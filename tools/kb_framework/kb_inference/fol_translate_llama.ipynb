{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1bb856-0273-45cb-b0b2-a72d8e0ee8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from typing import List\n",
    "import transformers\n",
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d82db29-08f1-4571-bb7f-d548026bd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to store the pipeline\n",
    "global_pipeline = None\n",
    "\n",
    "def load_llm():\n",
    "    global global_pipeline\n",
    "    # model_id = \"/data/models/LLaMa3/Meta-Llama-3-70B-Instruct-hf/\"\n",
    "    model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "    # model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
    "    # model_id = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "    # model_id = \"/data/models/LLaMa3/Meta-Llama-3.1-8B-Instruct-hf/\"\n",
    "    global_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    logger.info(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56089ae-7904-4d9b-a62a-ef42b968dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_knowledge_base(file_path: str) -> str:\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def generate_queries(question, kb_file, properties_file, example_file):\n",
    "    # Load knowledge base and properties\n",
    "    kb_content = load_knowledge_base(kb_file)\n",
    "    properties_content = load_knowledge_base(properties_file)\n",
    "    example_content = load_knowledge_base(example_file)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are an AI assistant tasked with generating queries for a first-order logic knowledge base using the AIMA Python library. Your task is to create a FOL query that can be used to answer the provided question.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    To understand how to translate and what are the available predicates, you can look into the following FOL predicates and statements.\n",
    "\n",
    "    Knowledge Base Rules:\n",
    "    {kb_content}\n",
    "\n",
    "    Example of FOL related to object properties:\n",
    "    {properties_content}\n",
    "\n",
    "    Some more examples and explanations:\n",
    "    TypeOf(x, Car)\n",
    "    ColorOf(x, Red)\n",
    "\n",
    "    TypeOf vehicles could be Car, Bus, Van, MiniCooper, MiniVan, Truck, Hatchback, Motorcycle, Bicycle, SUV, Jeep etc.\n",
    "    ColorOf vehicles could be any color that is associated with a vehicle.\n",
    "\n",
    "    Object properties take the type of vehicles and color. These can be retrieved from the question. But the color and type need\n",
    "    to be capitalized, and any space in it should be removed. For example, a Police car should be PoliceCar. For PoliceCar you do not need to add ColorOf the police car in the FOL. But for other vehicles include ColorOf.\n",
    "\n",
    "    Based on the knowledge base and object properties, generate a FOL query that would be necessary to answer the question. The queries should be in the format used by the AIMA Python library's fol_fc_ask function.\n",
    "\n",
    "    Rules for generating queries:\n",
    "    1. Each query should be a string that might be a conjunction of multiple predicates.\n",
    "    2. Use 'x' as the variable name for the main object in question.\n",
    "    3. The predicates should be combined to form a conjunctive query.\n",
    "    4. Include predicates for type, attributes, and relevant relationships.\n",
    "    5. Please do not use any predicate that is not present in the Knowledge Base Rules or Object Properties.\n",
    "    6. Please use the predicates that are present in the rules. Please do not make any changes in predicates.\n",
    "    7. Check the location of the object if the location is mentioned in the question.\n",
    "    8. Questions that involve a single object should be responded to with a query which is a conjunction of some predicates.\n",
    "    9. If the location of an object is not mentioned in the question then do not include InitialLocation or LastLocation predicates in the query.\n",
    "\n",
    "    For example. For Location, use InitialLocation(x, position) or LastLocation(x, position) predicates. Don't use only Location(x). Then, create another list of actions between the two objects.\n",
    "\n",
    "    Example 1:\n",
    "    Question: \"Is there a white car near the left?\"\n",
    "    Query: \"TypeOf(x, Car)^ColorOf(x, White)^InitialLocation(x, NearLeft)\"\n",
    "\n",
    "    10. If the question involves two objects, then first generate the FOL query for each object similar to a single object question.\n",
    "    11. Use 'y' as the variable name for the second object in the question.\n",
    "    12. For two objects, the response would be the conjunction of all the predicates for both objects, which contains the predicates for the first object, the predicates for the second object, and the predicates for the interaction between them (if available).\n",
    "    13. Only respond with the FOL query of the input question. Do not add any other text to it\n",
    "    Example 2:\n",
    "    Question: \"Does the white car near the left come close to a pedestrian at the front?\"\n",
    "    Queries: \"TypeOf(x, Car)^ColorOf(x, White)^InitialLocation(x, NearLeft)^Pedestrian(y)^InitialLocation(y, Front)^ComeClose(x, y)\"\n",
    "\n",
    "    Example 3:\n",
    "    Question: \"Can you spot a pedestrian walking near the right of the police car?\"\n",
    "    Queries: \"Pedestrian(x)^Walk(x)^InitialLocation(x, NearRight)^TypeOf(y, PoliceCar)\"\n",
    "\n",
    "    Example 4:\n",
    "    Question: \"Can you spot a pedestrian walking near the right of the police car at the center?\"\n",
    "    Queries: \"Pedestrian(x)^Walk(x)^InitialLocation(x, NearRight)^TypeOf(y, PoliceCar)^InitialLocation(x, Front)\"\n",
    "\n",
    "    In the above examples (example 3, 4), notice that both the examples mentioned about a police car. But in example 3, there was no mention of location of the police car, hence the FOL query does not include any location for the police car. But in the 4th example as there is a mention of the police car's position, we include it in the FOL query.\n",
    "\n",
    "    Location can be any of the following: Left, NearLeft, FarLeft, Right, NearRight, FarRight, Front, NearFront, and FarFront. Please analyze the question\n",
    "    Choose the proper one, and do not use any words other than the mentioned 9 for location. Do not use NearLeft, FarLeft, NearRight, FarRight, NearFront, and FarFront as the location predicate value if those are not mentioned explicitly in the question.\n",
    "    Inside the location  predicates (InitialLocation, LastLocation) please don't use any other location other than these six: NearLeft, FarLeft, NearRight, FarRight, NearFront, and FarFront\n",
    "    If a synonym is used in the question, try to find the closest one from the six mentioned positions.\n",
    "    If you are not sure about the location/position of an object, seeing the question, don't include location predicates for that object.\n",
    "    For example, the center can be replaced by NearFront.\n",
    "    For location, we only have two predicates. Example: InitialLocation(x, NearLeft) and LastLocation(x, NearLeft)\n",
    "\n",
    "    On(x, y) predicate signifies if object x is positioned on top of y. It does not signify if object x is at y side of the frmae.\n",
    "\n",
    "    Here are more examples showing questions and their corresponding FOL Queries:\n",
    "    {example_content}\n",
    "\n",
    "    Please provide a similar list of queries for the given question. Ensure that the queries cover all aspects necessary to answer the question based on the knowledge base and object properties.\n",
    "    Also, please consider that some true predicates can lead other predicates to be true. For example: \n",
    "\n",
    "    ((Vehicles(x) & SpeedUp(x)) ==> Accelerate(x))\n",
    "    ((Vehicles(x) & SpeedDown(x)) ==> Decelerate(x))\n",
    "    (((Vehicles(x) & NotAccelerate(x)) & NotDecelerate(x)) ==> ConstantSpeed(x))\n",
    "\n",
    "    Here, you do not need to check all the predicates (SpeedUp, SpeedDown, ConstantSpeed) to understand if a vehicle is moving at a constant speed.\n",
    "    You can only check ConstantSpeed(x).\n",
    "    \n",
    "    Your response should be a query string. For example:\n",
    "    \"Predicate1(x, Value)^Predicate2(x)^Predicate3(x, OtherValue)\"\n",
    "\n",
    "    Respond with only the query string, no additional text.\n",
    "    \"\"\"\n",
    "\n",
    "    # return prompt\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant specializing in generating logical queries for AIMA Python based on first-order logic knowledge bases and object properties.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        global_pipeline.tokenizer.eos_token_id,\n",
    "        global_pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    outputs = global_pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=1000,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        # temperature=0.6,\n",
    "        # top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    result = outputs[0][\"generated_text\"][-1]['content']\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff39999-4664-4f2d-b19c-2721a3bc0a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 20:33:54.970274: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741138434.983484 1872476 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741138434.987503 1872476 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-04 20:33:55.002932: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05683415b1804cb8a61397305b9406ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "INFO:__main__:Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "load_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2a9a179-5003-488e-8c5a-cd8c37bbfa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Found 96 already processed questions\n",
      "INFO:__main__:Processing 2 new questions\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:34<00:00, 17.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# def write_to_file(file_path, question, queries):\n",
    "#     with open(file_path, 'a', newline='') as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([question, queries])\n",
    "\n",
    "# def main():\n",
    "#     # kb_file = \"./fol_rules_kbs/out_017_181_191.txt\"\n",
    "#     random.seed(1)\n",
    "#     kb_file = \"/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/rules/all_rules.txt\"\n",
    "#     properties_file = \"/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/rules/normal_vehicle_info_frequent_0001.txt\"\n",
    "#     out_file_train = 'question_query_carla_llama3.csv'\n",
    "\n",
    "#     questions = pd.read_csv(\n",
    "#         '/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/questions/all_seq_qt_2_final.csv'\n",
    "#     )['Questions'].fillna('')\n",
    "#     questions = [q for q in questions if q != '']\n",
    "#     # random.shuffle(questions)\n",
    "\n",
    "#     with open(out_file_train, 'w', newline='') as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\"Question\", \"FOL Query\"])\n",
    "    \n",
    "#     # Process train data\n",
    "#     for question in tqdm(questions):\n",
    "#         queries = generate_queries(question, kb_file, properties_file)\n",
    "#         write_to_file(out_file_train, question, queries.replace('\"', '').replace(\"'\", ''))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "def write_to_file(file_path, question, queries):\n",
    "    with open(file_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([question, queries])\n",
    "\n",
    "def main():\n",
    "    kb_file = \"/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/rules/all_rules.txt\"\n",
    "    properties_file = \"/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/rules/normal_vehicle_info_frequent_0001.txt\"\n",
    "    out_file_train = '/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/kb_inference/translated_queries/question_query_kitti_llama33.csv'\n",
    "    example_file = '/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/kb_inference/examples/example_translation_1.txt'\n",
    "    \n",
    "    questions = pd.read_csv('/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/kitti_questions/all_que_fn.csv')['Questions'].fillna('')\n",
    "    questions = [q for q in questions if q != '']\n",
    "    \n",
    "    # Check if output file exists and load already processed questions\n",
    "    processed_questions = set()\n",
    "    if os.path.exists(out_file_train):\n",
    "        try:\n",
    "            existing_data = pd.read_csv(out_file_train)\n",
    "            processed_questions = set(existing_data['Question'].tolist())\n",
    "            logger.info(f\"Found {len(processed_questions)} already processed questions\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            logger.info(\"Existing file is empty, processing all questions\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading existing file: {e}\")\n",
    "            logger.info(\"Creating new output file\")\n",
    "            processed_questions = set()\n",
    "    \n",
    "    # Filter out already processed questions\n",
    "    remaining_questions = [q for q in questions if q not in processed_questions]\n",
    "    logger.info(f\"Processing {len(remaining_questions)} new questions\")\n",
    "    \n",
    "    # Create file if it doesn't exist, or append if it exists\n",
    "    if not os.path.exists(out_file_train):\n",
    "        with open(out_file_train, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Question\", \"FOL Query\"])\n",
    "    \n",
    "    # Process remaining questions\n",
    "    for i_cnt, question in enumerate(tqdm(remaining_questions)):\n",
    "        try:\n",
    "            queries = generate_queries(question, kb_file, properties_file, example_file)\n",
    "            write_to_file(out_file_train, question, queries.replace('\"', '').replace(\"'\", ''))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing question '{question}': {e}\")\n",
    "            continue\n",
    "\n",
    "        # time.sleep(5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
