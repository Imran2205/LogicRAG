{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14cf7c0f-b95f-4fd4-bfc2-cfafceb93a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T18:41:03.208065Z",
     "start_time": "2025-02-19T18:41:03.186912Z"
    }
   },
   "outputs": [],
   "source": [
    "from kanren import Relation, facts, run, var, eq, conde\n",
    "from kanren.core import lall\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Dict, Set\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class KanrenKB:\n",
    "    def __init__(self):\n",
    "        self.type_of = Relation('TypeOf')\n",
    "        self.color_of = Relation('ColorOf')\n",
    "        self.railtrack = Relation('Railtrack')\n",
    "        self.rules = []\n",
    "        self.antecedent_predicates = []\n",
    "        self.consequent_predicates = []\n",
    "       \n",
    "        self.relations = {\n",
    "            'TypeOf': self.type_of,\n",
    "            'ColorOf': self.color_of,\n",
    "            'Railtrack': self.railtrack\n",
    "        }\n",
    "\n",
    "    def _extract_predicates_from_kb(self, kb_filename: str) -> Set[str]:\n",
    "        \"\"\"Extract unique predicates from all lines in KB file\"\"\"\n",
    "        predicates = set()\n",
    "        \n",
    "        with open(kb_filename, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                if '==>' in line:\n",
    "                    # Handle rules: split and process both sides\n",
    "                    antecedent, consequent = line.split('==>')\n",
    "                    # Process both parts\n",
    "                    parts = [antecedent.strip('()'), consequent.strip('()')]\n",
    "                    for part in parts:\n",
    "                        matches = re.findall(r'([A-Za-z][A-Za-z0-9_]*)\\(', part)\n",
    "                        predicates.update(matches)\n",
    "                else:\n",
    "                    # Handle facts and other lines\n",
    "                    matches = re.findall(r'([A-Za-z][A-Za-z0-9_]*)\\(', line)\n",
    "                    predicates.update(matches)\n",
    "\n",
    "        # Remove predicates that are already defined\n",
    "        # predicates = predicates - set(self.relations.keys())\n",
    "        \n",
    "        logger.debug(f\"Found {len(predicates)} additional unique predicates\")\n",
    "        logger.debug(f\"Additional predicates: {sorted(predicates)}\")\n",
    "        return predicates\n",
    "\n",
    "    def _initialize_relations(self, predicates: Set[str]):\n",
    "        \"\"\"Create Relation objects for each predicate\"\"\"\n",
    "        for pred in predicates:\n",
    "            self.relations[pred] = Relation(pred)\n",
    "        logger.debug(f\"Total relations initialized: {len(self.relations)}\")\n",
    "        logger.debug(f\"Relations: {', '.join(sorted(self.relations.keys()))}\")\n",
    "\n",
    "    def load_kb(self, kb_filename: str, obj_info_filename: str):\n",
    "        \"\"\"Load knowledge base from files\"\"\"\n",
    "        logger.debug(\"Extracting predicates and initializing relations...\")\n",
    "        \n",
    "        # Extract predicates from both files\n",
    "        predicates = self._extract_predicates_from_kb(kb_filename)\n",
    "        obj_predicates = self._extract_predicates_from_kb(obj_info_filename)\n",
    "        predicates.update(obj_predicates)\n",
    "        \n",
    "        # Initialize additional relations\n",
    "        self._initialize_relations(predicates)\n",
    "        \n",
    "        logger.debug(\"Loading knowledge base...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load rules and facts from KB file\n",
    "        with open(kb_filename, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                    \n",
    "                if line.startswith('('):\n",
    "                    self._process_rule(line)\n",
    "                else:\n",
    "                    self._add_fact(line)\n",
    "\n",
    "        # Load object properties\n",
    "        with open(obj_info_filename, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    self._add_fact(line)\n",
    "                    # print(line)\n",
    "\n",
    "        self._add_rule()\n",
    "                    \n",
    "        logger.debug(f\"KB loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    def _process_rule(self, rule_str: str):\n",
    "\t    \"\"\"Process an implication rule\"\"\"\n",
    "\t    # Remove outer parentheses and split on ==>\n",
    "\t    rule = rule_str.lstrip('(')[:-1] # .strip('()')\n",
    "\t    \n",
    "\t    if '==>' in rule:\n",
    "\t        antecedent, consequent = rule.split('==>', 1)\n",
    "\t        \n",
    "\t        # Clean and parse both parts\n",
    "\t        def clean_and_parse(expr):\n",
    "\t            # Remove all parentheses\n",
    "\t            expr = expr.replace('((', '(').replace('))', ')')\n",
    "\t            # Split on & and clean each predicate\n",
    "\t            predicates = [p.strip() for p in expr.split('&')]\n",
    "\t            # Remove empty strings\n",
    "\t            predicates = [p for p in predicates if p]\n",
    "\t            return predicates\n",
    "\n",
    "\n",
    "\t        self.antecedent_predicates.append(clean_and_parse(antecedent))\n",
    "\t        self.consequent_predicates.append(clean_and_parse(consequent))\n",
    "\n",
    "            \n",
    "\t        # Debug output\n",
    "\t        # print(\"Rule:\", rule_str)\n",
    "\t        # print(\"Parsed predicates:\")\n",
    "\t        # print(\"Antecedent:\", len(self.antecedent_predicates))\n",
    "\t        # print(\"Consequent:\", len(self.consequent_predicates))\n",
    "\t        # print(\"-\" * 50)\n",
    "\t        \n",
    "\t        # self._add_rule(antecedent_predicates, consequent_predicates)\n",
    "        \n",
    "    def _add_rule(self):\n",
    "        \"\"\"Add a rule to the knowledge base with implication logic\"\"\"\n",
    "        x, y = var(), var()\n",
    "        \n",
    "        for pred_ind, antecedent_predicate in enumerate(self.antecedent_predicates):\n",
    "            # Get results from antecedent predicates\n",
    "            results = None\n",
    "            \n",
    "            # Process each antecedent predicate\n",
    "            for pred in antecedent_predicate:\n",
    "                pred_name = pred[:pred.index('(')]\n",
    "                \n",
    "                if pred_name not in self.relations:\n",
    "                    self.relations[pred_name] = Relation(pred_name)\n",
    "                    \n",
    "                args = pred[pred.index('(')+1:pred.rindex(')')].split(',')\n",
    "                args = [arg.strip() for arg in args]\n",
    "                \n",
    "                query_terms = []\n",
    "                for arg in args:\n",
    "                    if arg == 'x':\n",
    "                        query_terms.append(x)\n",
    "                    elif arg == 'y':\n",
    "                        query_terms.append(y)\n",
    "                    else:\n",
    "                        query_terms.append(arg)\n",
    "                \n",
    "                # For single argument predicates\n",
    "                if len(query_terms) == 1:\n",
    "                    current_results = set(run(0, x, self.relations[pred_name](x)))\n",
    "                else:  # For two argument predicates\n",
    "                    current_results = set(run(0, (x, y), self.relations[pred_name](x, y)))\n",
    "                \n",
    "                # print(f\"Results for {pred_name}: {current_results}\")\n",
    "                \n",
    "                if results is None:\n",
    "                    results = current_results\n",
    "                else:\n",
    "                    results = results.intersection(current_results)\n",
    "            \n",
    "            # Process consequent with the results\n",
    "            consequent_predicate = self.consequent_predicates[pred_ind]\n",
    "            for pred in consequent_predicate:\n",
    "                pred_name = pred[:pred.index('(')]\n",
    "                \n",
    "                if pred_name not in self.relations:\n",
    "                    self.relations[pred_name] = Relation(pred_name)\n",
    "                \n",
    "                args = pred[pred.index('(')+1:pred.rindex(')')].split(',')\n",
    "                args = [arg.strip() for arg in args]\n",
    "                \n",
    "                # print(f\"Adding consequent facts for {pred_name} with results: {results}\")\n",
    "                \n",
    "                # Add facts based on argument count\n",
    "                if len(args) == 1:  # Single variable predicate\n",
    "                    for result in results:\n",
    "                        # print(result)\n",
    "                        if isinstance(result, tuple):\n",
    "                            facts(self.relations[pred_name], (result[0],))\n",
    "                        else:\n",
    "                            facts(self.relations[pred_name], (result,))\n",
    "                            \n",
    "                    # print(f\"After adding facts, query result for {pred_name}:\", \n",
    "                    #   list(run(0, x, self.relations[pred_name](x))))\n",
    "                else:  # Two variable predicate\n",
    "                    for result in results:\n",
    "                        # print(result)\n",
    "                        facts(self.relations[pred_name], (result[0], result[1]))\n",
    "                        facts(self.relations[pred_name], (result[1], result[0]))\n",
    "                        \n",
    "                    # print(f\"After adding facts, query result for {pred_name}:\", \n",
    "                    #   list(run(0, (x, y), self.relations[pred_name](x, y))))\n",
    "                \n",
    "                \n",
    "\n",
    "    def _add_fact(self, fact_str: str):\n",
    "        \"\"\"Add a single fact to the knowledge base\"\"\"\n",
    "        try:\n",
    "            pred_name = fact_str[:fact_str.index('(')]\n",
    "            args = fact_str[fact_str.index('(')+1:fact_str.rindex(')')].split(',')\n",
    "            args = [arg.strip() for arg in args]\n",
    "            \n",
    "            if pred_name in self.relations:\n",
    "                # print(f\"**{pred_name}**\", [tuple(args)])\n",
    "                facts(self.relations[pred_name], tuple(args))\n",
    "            else:\n",
    "                logger.warning(f\"Unknown predicate in fact: {pred_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding fact '{fact_str}': {e}\")\n",
    "\n",
    "\n",
    "    def conjunctive_query(self, queries: List[str], var_name: str = 'x'):\n",
    "        \"\"\"Run a conjunctive query with multiple conditions\"\"\"\n",
    "        # Create variables for x and y\n",
    "        x, y = var(), var()\n",
    "        main_var = x if var_name == 'x' else y\n",
    "        \n",
    "        # Process queries into goals\n",
    "        goals = []\n",
    "        infer_sets = []\n",
    "        for query in queries:\n",
    "            pred_name = query[:query.index('(')]\n",
    "            args = query[query.index('(')+1:query.rindex(')')].split(',')\n",
    "            args = [arg.strip() for arg in args]\n",
    "            \n",
    "            # Skip if relation doesn't exist\n",
    "            if pred_name not in self.relations:\n",
    "                logger.warning(f\"Predicate {pred_name} not found in relations\")\n",
    "                continue\n",
    "                \n",
    "            # Build query terms\n",
    "            query_terms = []\n",
    "            for arg in args:\n",
    "                if arg == 'x':\n",
    "                    query_terms.append(x)\n",
    "                elif arg == 'y':\n",
    "                    query_terms.append(y)\n",
    "                else:\n",
    "                    query_terms.append(arg)\n",
    "            infer_sets.append(set(run(0, main_var, self.relations[pred_name](*query_terms))))\n",
    "            goals.append(self.relations[pred_name](*query_terms))\n",
    "        \n",
    "        # Run the query with all conditions\n",
    "        results = set.intersection(*infer_sets)\n",
    "\n",
    "        # return list(results)\n",
    "\n",
    "        # print(infer_sets)\n",
    "        if goals:\n",
    "            results = run(0, main_var, lall(*goals))\n",
    "            # print(results)\n",
    "            return list(results)\n",
    "        return []\n",
    "    \n",
    "    def kb_query(self, queries: List[List[str]]):\n",
    "        \"\"\"Handle queries involving multiple objects and their relationships\"\"\"\n",
    "        # Get results for first object (x)\n",
    "\n",
    "        if len(queries[1]) == 0 or len(queries[2]) == 0:\n",
    "            x_results = self.conjunctive_query(queries[0], 'x')\n",
    "            logger.debug(f\"Found {len(x_results)} matches for first object\")\n",
    "            return x_results\n",
    "        \n",
    "        x_results = self.conjunctive_query(queries[0], 'x')\n",
    "        logger.debug(f\"Found {len(x_results)} matches for first object\")\n",
    "        \n",
    "        # Get results for second object (y)\n",
    "        y_results = self.conjunctive_query(queries[1], 'y')\n",
    "        logger.debug(f\"Found {len(y_results)} matches for second object\")\n",
    "        \n",
    "        if not x_results or not y_results:\n",
    "            return []\n",
    "        \n",
    "        # Check relationship between objects\n",
    "        final_results = []\n",
    "        for x_obj in x_results:\n",
    "            for y_obj in y_results:\n",
    "                # For each relationship query\n",
    "                all_relationships_match = True\n",
    "                for relationship_query in queries[2]:\n",
    "                    pred_name = relationship_query[:relationship_query.index('(')]\n",
    "                    # print(pred_name)\n",
    "                    if pred_name not in self.relations:\n",
    "                        logger.warning(f\"Relationship predicate {pred_name} not found\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Create a specific query for this pair of objects\n",
    "                    v = var()  # dummy variable for run query\n",
    "                    # print(pred_name, x_obj, y_obj)\n",
    "                    result = run(1, v, self.relations[pred_name](x_obj, y_obj))\n",
    "                    # print(result)\n",
    "                    \n",
    "                    if not result:\n",
    "                        all_relationships_match = False\n",
    "                        break\n",
    "                \n",
    "                if all_relationships_match:\n",
    "                    final_results.append((x_obj, y_obj))\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "def parse_query(query_str):\n",
    "    \"\"\"Convert query string to grouped lists of predicates\"\"\"\n",
    "    # Split on ^ to get individual predicates\n",
    "    predicates = query_str.split('^')\n",
    "    \n",
    "    # Clean predicates and group them\n",
    "    x_predicates = []\n",
    "    y_predicates = []\n",
    "    xy_predicates = []\n",
    "    \n",
    "    for pred in predicates:\n",
    "        pred = pred.strip()\n",
    "        \n",
    "        # Skip empty predicates\n",
    "        if not pred:\n",
    "            continue\n",
    "            \n",
    "        # Check predicate arguments\n",
    "        args = pred[pred.index('(')+1:pred.rindex(')')].split(',')\n",
    "        args = [arg.strip() for arg in args]\n",
    "        \n",
    "        # Group based on variables present\n",
    "        if 'x' in args and 'y' in args:\n",
    "            xy_predicates.append(pred)\n",
    "        elif 'x' in args:\n",
    "            x_predicates.append(pred)\n",
    "        elif 'y' in args:\n",
    "            y_predicates.append(pred)\n",
    "    \n",
    "    return [x_predicates, y_predicates, xy_predicates]\n",
    "\n",
    "def main(kb_file, obj_info_file, queries):\n",
    "    kb = KanrenKB()\n",
    "    kb.load_kb(kb_file, obj_info_file)\n",
    "\n",
    "    answers_ = []\n",
    "\n",
    "    for query in queries:\n",
    "        query_list = parse_query(query)\n",
    "    \n",
    "        results = kb.kb_query(query_list)\n",
    "\n",
    "        # print(results)\n",
    "\n",
    "        if len(results) > 0:\n",
    "            answers_.append(1)\n",
    "        else:\n",
    "            answers_.append(0)\n",
    "\n",
    "    return answers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e546ac5-ede1-4735-9e3c-d79054ae7e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_sequences(questions_csv_path, fol_mapping_csv_path, video_id, dataset='carla'):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Read the CSVs\n",
    "    questions_df = pd.read_csv(questions_csv_path)\n",
    "    fol_mapping_df = pd.read_csv(fol_mapping_csv_path)\n",
    "    \n",
    "    # Create a mapping of questions to FOL queries\n",
    "    fol_dict = dict(zip(fol_mapping_df['Question'], fol_mapping_df['FOL Query']))\n",
    "    \n",
    "    # Initialize lists to store frame sequences\n",
    "    frame_groups = []\n",
    "    current_frame = None\n",
    "    current_questions = []\n",
    "    current_annotations = []\n",
    "\n",
    "    # tot_q = 0\n",
    "    # tot_cor = 0\n",
    "    seq_all_ans = []\n",
    "    seq_all_annos = []\n",
    "    \n",
    "    # Find all rows indices where Video number is present\n",
    "    video_start_indices = questions_df[pd.notna(questions_df['Video'])].index.tolist()\n",
    "    \n",
    "    # Process each video section\n",
    "    for i in range(len(video_start_indices)):\n",
    "        start_idx = video_start_indices[i]\n",
    "        # If this is not the last video section, get end index from next video start\n",
    "        if i < len(video_start_indices) - 1:\n",
    "            end_idx = video_start_indices[i + 1]\n",
    "        else:\n",
    "            end_idx = len(questions_df)\n",
    "            \n",
    "        # Get the video number for this section\n",
    "        section_video = questions_df.iloc[start_idx]['Video']\n",
    "        \n",
    "        # If this is our target video, process this section\n",
    "        if section_video == int(video_id):\n",
    "            # Get all rows for this section\n",
    "            section_rows = questions_df.iloc[start_idx:end_idx]\n",
    "            \n",
    "            # Process the frame sequence\n",
    "            frame_seq = section_rows.iloc[0]['Frame']\n",
    "            questions = []\n",
    "            annotations = []\n",
    "            \n",
    "            # Collect all valid questions and annotations from this section\n",
    "            for _, row in section_rows.iterrows():\n",
    "                if pd.notna(row['Questions']):\n",
    "                    questions.append(row['Questions'])\n",
    "                    annotations.append(int(row['Human']))\n",
    "            \n",
    "            if questions:  # Only add if we have questions\n",
    "                frame_groups.append((frame_seq, questions, annotations))\n",
    "\n",
    "    # print(frame_groups)\n",
    "    # Generate function calls for each frame sequence\n",
    "    for frame_seq, questions, annotations in frame_groups:\n",
    "        # Parse frame range\n",
    "        # start_frame, end_frame = map(int, frame_seq.split('-'))\n",
    "        start_frame, end_frame = frame_seq.split('-')\n",
    "        \n",
    "        # Generate FOL queries list\n",
    "        fol_queries = []\n",
    "        for question in questions:\n",
    "            if question in fol_dict:\n",
    "                fol_queries.append(fol_dict[question])\n",
    "            else:\n",
    "                print(f\"Warning: No FOL query found for question: {question}\")\n",
    "        \n",
    "        # Generate paths\n",
    "        kb_file_path = f\"/home/ibk5106/projects/projects/LogicRAG/kb_out_{dataset}/{video_id}/kb_window_{start_frame}_{end_frame}.txt\"\n",
    "        obj_info_file_path = f\"/home/ibk5106/projects/projects/LogicRAG/track_out_{dataset}/{video_id}/vehicles_prop.txt\"\n",
    "        \n",
    "        # Generate and print the function call\n",
    "        # print(f\"\\n# Video: {video_id}, Frame sequence: {frame_seq}\")\n",
    "        # print(fol_queries)\n",
    "\n",
    "        answers = main(\n",
    "            kb_file_path,\n",
    "            obj_info_file_path,\n",
    "            fol_queries\n",
    "        )\n",
    "\n",
    "        seq_all_ans += answers\n",
    "        seq_all_annos += annotations\n",
    "        \n",
    "        # print(\"GT:   \", annotations)\n",
    "        # print(\"PRED: \", answers)\n",
    "\n",
    "        # for i_ans, ans in enumerate(answers):\n",
    "        #     tot_q += 1\n",
    "        #     if ans == annotations[i_ans]:\n",
    "        #         tot_cor += 1\n",
    "        \n",
    "\n",
    "        # print(f\"# Questions in this sequence ({frame_seq}):\")\n",
    "        # for i, q in enumerate(questions):\n",
    "        #     print(f\"#  {i+1}. {q} (Ground truth: {annotations[i]})\")\n",
    "        # # print(f\"kb_file_path = \\\"{kb_file_path}\\\"\")\n",
    "        # # print(f\"obj_info_file_path = \\\"{obj_info_file_path}\\\"\")\n",
    "        # print(\"fol_queries = [\")\n",
    "        # for query in fol_queries:\n",
    "        #     print(f\"    \\\"{query}\\\",\")\n",
    "        # print(\"]\")\n",
    "        # print(f\"ground_truth = {annotations}\")\n",
    "        # print(f\"ans = {answers}\")\n",
    "\n",
    "    return seq_all_ans, seq_all_annos  # (tot_cor/tot_q)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138cf577-c2c9-4430-b4b4-457135ca09b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(all_answers_, all_annotations_):\n",
    "    tot_q = 0\n",
    "    tot_cor = 0\n",
    "    for i_ans, ans in enumerate(all_answers_):\n",
    "        tot_q += 1\n",
    "        if ans == all_annotations_[i_ans]:\n",
    "            tot_cor += 1\n",
    "    \n",
    "    y_true = np.array(all_annotations_)\n",
    "    y_pred = np.array(all_answers_)\n",
    "    \n",
    "    f1_ = f1_score(y_true, y_pred)\n",
    "    \n",
    "    precision_ = precision_score(y_true, y_pred)\n",
    "    recall_ = recall_score(y_true, y_pred)\n",
    "\n",
    "    accuracy_ = tot_cor/tot_q\n",
    "\n",
    "    return accuracy_, f1_, precision_, recall_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f1e4c29-265d-4e06-8ad2-f89f9e12854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_ids = ['0000', '0001', '0002', '0003', '0004', '0005', '0008', '0010', '0012']\n",
    "qs_csv_path = \"/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/kitti_questions/all_que_fn.csv\"\n",
    "# fol_trns_csv_path = \"/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/kb_inference/question_query_kitti_claude.csv\"\n",
    "fol_trns_csv_path = \"/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/kb_inference/translated_queries/question_query_kitti_llama33.csv\"\n",
    "\n",
    "all_answers = []\n",
    "all_annotations = []\n",
    "\n",
    "for v_id in vid_ids:\n",
    "    ans, annos = process_video_sequences(\n",
    "        qs_csv_path, fol_trns_csv_path, v_id, dataset='kitti'\n",
    "    )\n",
    "    \n",
    "    all_answers += ans\n",
    "    all_annotations += annos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71095474-69c7-406f-845f-f81bbd09c291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.94, F1: 0.97, Prec: 0.97, Rec: 0.97\n"
     ]
    }
   ],
   "source": [
    "tot_q = 0\n",
    "tot_cor = 0\n",
    "for i_ans, ans in enumerate(all_answers):\n",
    "    tot_q += 1\n",
    "    if ans == all_annotations[i_ans]:\n",
    "        tot_cor += 1\n",
    "\n",
    "y_true = np.array(all_annotations)\n",
    "y_pred = np.array(all_answers)\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Overall Accuracy: {tot_cor/tot_q:.2f}, F1: {f1:.2f}, Prec: {precision:.2f}, Rec: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b95ac77-bece-47ea-b108-d11c44fcd43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/kitti_questions/kitti_que_all_model_ans.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "111ddbd5-1199-445c-80ed-d1eb5225a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['LogicRAG'] = all_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6526f85-2a4f-4474-9183-d2fbbf5b5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"/home/ibk5106/projects/projects/LogicRAG/tools/kb_framework/kitti_questions/kitti_que_all_model_ans.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e736a7-d24b-4246-a250-50593de136eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.74, F1: 0.84, Prec: 0.96, Rec: 0.75\n"
     ]
    }
   ],
   "source": [
    "all_annotations = list(df['Human'])\n",
    "all_answers = list(df['claude-3-5-sonnet-20240620'])\n",
    "\n",
    "acc, f1, precision, recall = get_scores(all_answers, all_annotations)\n",
    "\n",
    "print(f\"Overall Accuracy: {acc:.2f}, F1: {f1:.2f}, Prec: {precision:.2f}, Rec: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95ac8142-f32a-41f5-9e55-7575e9d5826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.71, F1: 0.83, Prec: 0.92, Rec: 0.75\n"
     ]
    }
   ],
   "source": [
    "all_annotations = list(df['Human'])\n",
    "all_answers = list(df['gpt-4o'])\n",
    "\n",
    "acc, f1, precision, recall = get_scores(all_answers, all_annotations)\n",
    "\n",
    "print(f\"Overall Accuracy: {acc:.2f}, F1: {f1:.2f}, Prec: {precision:.2f}, Rec: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5559198-2464-42aa-ae6a-f0d05b47c1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.92, F1: 0.96, Prec: 0.99, Rec: 0.92\n"
     ]
    }
   ],
   "source": [
    "all_annotations = list(df['Human'])\n",
    "all_answers = list(df['LogicRAG'])\n",
    "\n",
    "acc, f1, precision, recall = get_scores(all_answers, all_annotations)\n",
    "\n",
    "print(f\"Overall Accuracy: {acc:.2f}, F1: {f1:.2f}, Prec: {precision:.2f}, Rec: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b3a9c-4e69-4049-9a0e-7e3df6694f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
